<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>fusion.dataset.mnist_svhn.mnist_svhn API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fusion.dataset.mnist_svhn.mnist_svhn</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import copy
import os
from typing import Any, Dict, List, Union

from sklearn.model_selection import StratifiedKFold
import torch
import torchvision
from torch import Tensor
from torch.utils.data import DataLoader, Dataset
from torchnet.dataset import TensorDataset, ResampleDataset

from fusion.dataset.utils import seed_worker
from fusion.dataset.abasedataset import ABaseDataset, SetId
from fusion.dataset.mnist_svhn.transforms import SVHNTransform, MNISTTransform


class MnistSvhn(ABaseDataset):
    def __init__(
            self,
            dataset_dir: str,
            fold: int = 0,
            num_folds: int = 5,
            sources: List[int] = [0],
            batch_size: int = 2,
            shuffle: bool = False,
            drop_last: bool = False,
            num_workers: int = 0,
            seed: int = 343,
    ):
        &#34;&#34;&#34;
        Initialization of Class MnistSvhn dataset
        Args:
            dataset_dir: path to dataset
            fold: number of fold for validation
            num_folds: counts of folds
            views: number of views
            batch_size: how many samples per batch to load
            shuffle: set to True to have the data reshuffled at every epoch
            drop_last: set to True to drop the last incomplete batch
            num_workers: how many subprocesses to use for data loading
            seed: number of seed
        Return:
            Dataset MnistSvhn

        &#34;&#34;&#34;
        super().__init__(
            dataset_dir,
            fold=fold,
            num_folds=num_folds,
            sources=sources,
            batch_size=batch_size,
            shuffle=shuffle,
            drop_last=drop_last,
            num_workers=num_workers,
            seed=seed,
        )
        self._sources = sources
        self._indexes: Dict[str, Dict[str, Any]] = {}

    def load(self):
        &#34;&#34;&#34;
        Method to load dataset
        &#34;&#34;&#34;
        if os.path.exists(self._dataset_dir):
            self._download_dataset(self._dataset_dir)
        self._num_classes = 10
        # Don&#39;t touch it, otherwise lazy evaluation and lambda functions will make you cry
        samplers = {
            &#39;mnist&#39;: {
                SetId.TRAIN: lambda d, i: self._indexes[SetId.TRAIN][&#39;mnist&#39;][i],
                SetId.VALID: lambda d, i: self._indexes[SetId.VALID][&#39;mnist&#39;][i],
                SetId.TEST: lambda d, i: self._indexes[SetId.TEST][&#39;mnist&#39;][i],
            },
            &#39;svhn&#39;: {
                SetId.TRAIN: lambda d, i: self._indexes[SetId.TRAIN][&#39;svhn&#39;][i],
                SetId.VALID: lambda d, i: self._indexes[SetId.VALID][&#39;svhn&#39;][i],
                SetId.TEST: lambda d, i: self._indexes[SetId.TEST][&#39;svhn&#39;][i],
            }
        }

        for set_id in [SetId.TRAIN, SetId.VALID, SetId.TEST]:
            dataset = None
            sampler_mnist = samplers[&#39;mnist&#39;][set_id]
            sampler_svhn = samplers[&#39;svhn&#39;][set_id]
            if len(self._sources) == 2:
                dataset_mnist, indexes_mnist = self._load(set_id, &#39;mnist&#39;)
                dataset_svhn, indexes_svhn = self._load(set_id, &#39;svhn&#39;)
                self._indexes[set_id] = {}
                self._indexes[set_id][&#39;mnist&#39;] = indexes_mnist
                self._indexes[set_id][&#39;svhn&#39;] = indexes_svhn
                dataset = TensorDataset([
                    ResampleDataset(
                        dataset_mnist.dataset,
                        sampler_mnist,
                        size=len(self._indexes[set_id][&#39;mnist&#39;])
                    ),
                    ResampleDataset(
                        dataset_svhn.dataset,
                        sampler_svhn,
                        size=len(self._indexes[set_id][&#39;svhn&#39;])
                    )
                ])
                # collate_fn or tensor dataset with transforms
            else:
                if self._sources[0] == 0:
                    dataset_mnist, indexes_mnist = self._load(set_id, &#39;mnist&#39;)
                    self._indexes[set_id] = {}
                    self._indexes[set_id][&#39;mnist&#39;] = indexes_mnist
                    dataset = TensorDataset([
                        ResampleDataset(
                            dataset_mnist.dataset,
                            sampler_mnist,
                            size=len(indexes_mnist)
                        ),
                    ])
                elif self._sources[0] == 1:
                    self._indexes[set_id] = {}
                    dataset_svhn, indexes_svhn = self._load(set_id, &#39;svhn&#39;)
                    self._indexes[set_id][&#39;svhn&#39;] = indexes_svhn
                    dataset = TensorDataset([
                        ResampleDataset(
                            dataset_svhn.dataset,
                            sampler_svhn,
                            size=len(indexes_svhn)
                        )
                    ])
            self._set_dataloader(dataset, set_id)


    def _load(self, set_id: SetId, dataset_name: str):
        # define filename for pair indexes
        if set_id != SetId.TEST:
            filename = f&#34;{set_id.lower()}-ms-{dataset_name}-idx-{self._fold}.pt&#34;
        else:
            filename = f&#34;{set_id.lower()}-ms-{dataset_name}-idx.pt&#34;
        # load paired indexes
        indexes = torch.load(os.path.join(self._dataset_dir, filename))
        # load dataset
        if dataset_name == &#39;mnist&#39;:
            # validation uses training set
            train = True if set_id != SetId.TEST else False
            tx = MNISTTransform()
            dataset = torchvision.datasets.MNIST(
                self._dataset_dir, train=train, download=False, transform=tx)
        elif dataset_name == &#39;svhn&#39;:
            # validation uses training set
            split = SetId.TRAIN if set_id != SetId.TEST else SetId.TEST
            tx = SVHNTransform()
            dataset = torchvision.datasets.SVHN(
                self._dataset_dir, split=split, download=False, transform=tx)
        else:
            raise NotImplementedError
        # select fold
        if set_id != SetId.TEST:
            cv_indexes = torch.load(
                os.path.join(
                    self._dataset_dir,
                    f&#34;{set_id.lower()}-ms-{dataset_name}-cv-idx-{self._fold}.pt&#34;
                )
            )
            dataset.data = dataset.data[cv_indexes]
            if dataset_name == &#39;mnist&#39;:
                dataset.targets = dataset.targets[cv_indexes]
            elif dataset_name == &#39;svhn&#39;:
                dataset.labels = dataset.labels[cv_indexes]
            else:
                raise NotImplementedError
        dataset = DataLoader(
            dataset, batch_size=1, shuffle=False,
            pin_memory=True, num_workers=1,
            worker_init_fn=seed_worker,
        )
        return dataset, indexes

    def _set_dataloader(self, dataset: Dataset, set_id: SetId):
        data_loader = DataLoader(
            dataset,
            batch_size=self._batch_size,
            shuffle=self._shuffle,
            drop_last=self._drop_last,
            num_workers=self._num_workers,
            worker_init_fn=seed_worker,
        )
        set_id = SetId.INFER if set_id == SetId.TEST else set_id
        self._data_loaders[set_id] = data_loader

    def _set_num_classes(self, targets: Tensor):
        self._num_classes = len(torch.unique(targets))

    def _prepare_fold(
            self, dataset: Union[torchvision.datasets.MNIST, torchvision.datasets.SVHN],
            dataset_name: str
    ):
        kf = StratifiedKFold(
            n_splits=self._num_folds,
            shuffle=self._shuffle,
            random_state=self._seed
        )
        if dataset_name == &#39;MNIST&#39;:
            X, y = dataset.data, dataset.targets
        else:
            X, y = dataset.data, dataset.labels
        kf_g = kf.split(X, y)
        for _ in range(1, self._fold): next(kf_g)
        train_index, valid_index = next(kf.split(X, y))
        return train_index, valid_index

    def get_all_loaders(self):
        &#34;&#34;&#34;
        Return all loaders
        &#34;&#34;&#34;
        return super().get_all_loaders()

    def get_cv_loaders(self):
        &#34;&#34;&#34;
        Return all cross-validation loaders
        &#34;&#34;&#34;
        return super().get_cv_loaders()

    def get_loader(self, set_id: SetId):
        &#34;&#34;&#34;
        Get loader by set_id
        Args:
            set_id:
        Return:
            Loader by set_id
        &#34;&#34;&#34;
        return super().get_loader(set_id)

    def num_classes(self):
        return super().num_classes

    @staticmethod
    def _rand_match_on_idx(l1, idx1, l2, idx2, max_d: int = 10000, dm: int = 10):
        &#34;&#34;&#34;
        l*: sorted labels
        idx*: indices of sorted labels in original list
        &#34;&#34;&#34;
        _idx1, _idx2 = [], []
        for l in l1.unique():  # assuming both have same idxs
            l_idx1, l_idx2 = idx1[l1 == l], idx2[l2 == l]
            n = min(l_idx1.size(0), l_idx2.size(0), max_d)
            l_idx1, l_idx2 = l_idx1[:n], l_idx2[:n]
            for _ in range(dm):
                _idx1.append(l_idx1[torch.randperm(n)])
                _idx2.append(l_idx2[torch.randperm(n)])
        return torch.cat(_idx1), torch.cat(_idx2)

    def _download_dataset(self, dataset_dir: str):
        max_d = 10000  # maximum number of datapoints per class
        dm = 30  # data multiplier: random permutations to match

        # get the individual datasets
        tx = torchvision.transforms.ToTensor()
        if os.path.exists(self._dataset_dir):
            download = False
        else:
            download = True
            os.mkdir(self._dataset_dir)
        # load mnist
        train_mnist = torchvision.datasets.MNIST(
            dataset_dir, train=True, download=download, transform=tx)
        test_mnist = torchvision.datasets.MNIST(
            dataset_dir, train=False, download=download, transform=tx)

        # load svhn
        train_svhn = torchvision.datasets.SVHN(
            self._dataset_dir,
            split=&#34;train&#34;, download=download, transform=tx)
        test_svhn = torchvision.datasets.SVHN(
            self._dataset_dir,
            split=SetId.TEST, download=download, transform=tx)

        # svhn labels need extra work
        train_svhn.labels = torch.LongTensor(
            train_svhn.labels.squeeze().astype(int)) % 10
        test_svhn.labels = torch.LongTensor(
            test_svhn.labels.squeeze().astype(int)) % 10

        # split on cross-validation folds
        mnist_train_idxs, mnist_valid_idxs = self._prepare_fold(train_mnist, &#39;MNIST&#39;)
        svhn_train_idxs, svhn_valid_idxs = self._prepare_fold(train_svhn, &#39;SVHN&#39;)

        # save and pair training set
        mnist_l, mnist_li = train_mnist.targets[mnist_train_idxs].sort()
        svhn_l, svhn_li = train_svhn.labels[svhn_train_idxs].sort()
        idx1, idx2 = self._rand_match_on_idx(
            mnist_l, mnist_li, svhn_l, svhn_li, max_d=max_d, dm=dm)
        torch.save(idx1, os.path.join(
            dataset_dir, f&#34;train-ms-mnist-idx-{self._fold}.pt&#34;))
        torch.save(idx2, os.path.join(
            dataset_dir, f&#34;train-ms-svhn-idx-{self._fold}.pt&#34;))
        torch.save(mnist_train_idxs,
                   os.path.join(
                       dataset_dir, f&#39;train-ms-mnist-cv-idx-{self._fold}.pt&#39;))
        torch.save(svhn_train_idxs,
                   os.path.join(
                       dataset_dir, f&#39;train-ms-svhn-cv-idx-{self._fold}.pt&#39;))

        # save and pair validation set
        mnist_l, mnist_li = train_mnist.targets[mnist_valid_idxs].sort()
        svhn_l, svhn_li = train_svhn.labels[svhn_valid_idxs].sort()
        idx1, idx2 = self._rand_match_on_idx(
            mnist_l, mnist_li, svhn_l, svhn_li, max_d=max_d, dm=dm)
        torch.save(idx1, os.path.join(
            dataset_dir, f&#39;valid-ms-mnist-idx-{self._fold}.pt&#39;))
        torch.save(idx2, os.path.join(
            dataset_dir, f&#39;valid-ms-svhn-idx-{self._fold}.pt&#39;))
        torch.save(mnist_valid_idxs,
                   os.path.join(
                       dataset_dir, f&#39;valid-ms-mnist-cv-idx-{self._fold}.pt&#39;))
        torch.save(svhn_valid_idxs,
                   os.path.join(
                       dataset_dir,f&#39;valid-ms-svhn-cv-idx-{self._fold}.pt&#39;))

        # save and pair test set
        mnist_l, mnist_li = test_mnist.targets.sort()
        svhn_l, svhn_li = test_svhn.labels.sort()
        idx1, idx2 = self._rand_match_on_idx(
            mnist_l, mnist_li, svhn_l, svhn_li, max_d=max_d, dm=dm)
        torch.save(idx1, os.path.join(dataset_dir, &#39;test-ms-mnist-idx.pt&#39;))
        torch.save(idx2, os.path.join(dataset_dir, &#39;test-ms-svhn-idx.pt&#39;))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn"><code class="flex name class">
<span>class <span class="ident">MnistSvhn</span></span>
<span>(</span><span>dataset_dir: str, fold: int = 0, num_folds: int = 5, sources: List[int] = [0], batch_size: int = 2, shuffle: bool = False, drop_last: bool = False, num_workers: int = 0, seed: int = 343)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>Initialization of Class MnistSvhn dataset</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_dir</code></strong></dt>
<dd>path to dataset</dd>
<dt><strong><code>fold</code></strong></dt>
<dd>number of fold for validation</dd>
<dt><strong><code>num_folds</code></strong></dt>
<dd>counts of folds</dd>
<dt><strong><code>views</code></strong></dt>
<dd>number of views</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>how many samples per batch to load</dd>
<dt><strong><code>shuffle</code></strong></dt>
<dd>set to True to have the data reshuffled at every epoch</dd>
<dt><strong><code>drop_last</code></strong></dt>
<dd>set to True to drop the last incomplete batch</dd>
<dt><strong><code>num_workers</code></strong></dt>
<dd>how many subprocesses to use for data loading</dd>
<dt><strong><code>seed</code></strong></dt>
<dd>number of seed</dd>
</dl>
<h2 id="return">Return</h2>
<p>Dataset MnistSvhn</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MnistSvhn(ABaseDataset):
    def __init__(
            self,
            dataset_dir: str,
            fold: int = 0,
            num_folds: int = 5,
            sources: List[int] = [0],
            batch_size: int = 2,
            shuffle: bool = False,
            drop_last: bool = False,
            num_workers: int = 0,
            seed: int = 343,
    ):
        &#34;&#34;&#34;
        Initialization of Class MnistSvhn dataset
        Args:
            dataset_dir: path to dataset
            fold: number of fold for validation
            num_folds: counts of folds
            views: number of views
            batch_size: how many samples per batch to load
            shuffle: set to True to have the data reshuffled at every epoch
            drop_last: set to True to drop the last incomplete batch
            num_workers: how many subprocesses to use for data loading
            seed: number of seed
        Return:
            Dataset MnistSvhn

        &#34;&#34;&#34;
        super().__init__(
            dataset_dir,
            fold=fold,
            num_folds=num_folds,
            sources=sources,
            batch_size=batch_size,
            shuffle=shuffle,
            drop_last=drop_last,
            num_workers=num_workers,
            seed=seed,
        )
        self._sources = sources
        self._indexes: Dict[str, Dict[str, Any]] = {}

    def load(self):
        &#34;&#34;&#34;
        Method to load dataset
        &#34;&#34;&#34;
        if os.path.exists(self._dataset_dir):
            self._download_dataset(self._dataset_dir)
        self._num_classes = 10
        # Don&#39;t touch it, otherwise lazy evaluation and lambda functions will make you cry
        samplers = {
            &#39;mnist&#39;: {
                SetId.TRAIN: lambda d, i: self._indexes[SetId.TRAIN][&#39;mnist&#39;][i],
                SetId.VALID: lambda d, i: self._indexes[SetId.VALID][&#39;mnist&#39;][i],
                SetId.TEST: lambda d, i: self._indexes[SetId.TEST][&#39;mnist&#39;][i],
            },
            &#39;svhn&#39;: {
                SetId.TRAIN: lambda d, i: self._indexes[SetId.TRAIN][&#39;svhn&#39;][i],
                SetId.VALID: lambda d, i: self._indexes[SetId.VALID][&#39;svhn&#39;][i],
                SetId.TEST: lambda d, i: self._indexes[SetId.TEST][&#39;svhn&#39;][i],
            }
        }

        for set_id in [SetId.TRAIN, SetId.VALID, SetId.TEST]:
            dataset = None
            sampler_mnist = samplers[&#39;mnist&#39;][set_id]
            sampler_svhn = samplers[&#39;svhn&#39;][set_id]
            if len(self._sources) == 2:
                dataset_mnist, indexes_mnist = self._load(set_id, &#39;mnist&#39;)
                dataset_svhn, indexes_svhn = self._load(set_id, &#39;svhn&#39;)
                self._indexes[set_id] = {}
                self._indexes[set_id][&#39;mnist&#39;] = indexes_mnist
                self._indexes[set_id][&#39;svhn&#39;] = indexes_svhn
                dataset = TensorDataset([
                    ResampleDataset(
                        dataset_mnist.dataset,
                        sampler_mnist,
                        size=len(self._indexes[set_id][&#39;mnist&#39;])
                    ),
                    ResampleDataset(
                        dataset_svhn.dataset,
                        sampler_svhn,
                        size=len(self._indexes[set_id][&#39;svhn&#39;])
                    )
                ])
                # collate_fn or tensor dataset with transforms
            else:
                if self._sources[0] == 0:
                    dataset_mnist, indexes_mnist = self._load(set_id, &#39;mnist&#39;)
                    self._indexes[set_id] = {}
                    self._indexes[set_id][&#39;mnist&#39;] = indexes_mnist
                    dataset = TensorDataset([
                        ResampleDataset(
                            dataset_mnist.dataset,
                            sampler_mnist,
                            size=len(indexes_mnist)
                        ),
                    ])
                elif self._sources[0] == 1:
                    self._indexes[set_id] = {}
                    dataset_svhn, indexes_svhn = self._load(set_id, &#39;svhn&#39;)
                    self._indexes[set_id][&#39;svhn&#39;] = indexes_svhn
                    dataset = TensorDataset([
                        ResampleDataset(
                            dataset_svhn.dataset,
                            sampler_svhn,
                            size=len(indexes_svhn)
                        )
                    ])
            self._set_dataloader(dataset, set_id)


    def _load(self, set_id: SetId, dataset_name: str):
        # define filename for pair indexes
        if set_id != SetId.TEST:
            filename = f&#34;{set_id.lower()}-ms-{dataset_name}-idx-{self._fold}.pt&#34;
        else:
            filename = f&#34;{set_id.lower()}-ms-{dataset_name}-idx.pt&#34;
        # load paired indexes
        indexes = torch.load(os.path.join(self._dataset_dir, filename))
        # load dataset
        if dataset_name == &#39;mnist&#39;:
            # validation uses training set
            train = True if set_id != SetId.TEST else False
            tx = MNISTTransform()
            dataset = torchvision.datasets.MNIST(
                self._dataset_dir, train=train, download=False, transform=tx)
        elif dataset_name == &#39;svhn&#39;:
            # validation uses training set
            split = SetId.TRAIN if set_id != SetId.TEST else SetId.TEST
            tx = SVHNTransform()
            dataset = torchvision.datasets.SVHN(
                self._dataset_dir, split=split, download=False, transform=tx)
        else:
            raise NotImplementedError
        # select fold
        if set_id != SetId.TEST:
            cv_indexes = torch.load(
                os.path.join(
                    self._dataset_dir,
                    f&#34;{set_id.lower()}-ms-{dataset_name}-cv-idx-{self._fold}.pt&#34;
                )
            )
            dataset.data = dataset.data[cv_indexes]
            if dataset_name == &#39;mnist&#39;:
                dataset.targets = dataset.targets[cv_indexes]
            elif dataset_name == &#39;svhn&#39;:
                dataset.labels = dataset.labels[cv_indexes]
            else:
                raise NotImplementedError
        dataset = DataLoader(
            dataset, batch_size=1, shuffle=False,
            pin_memory=True, num_workers=1,
            worker_init_fn=seed_worker,
        )
        return dataset, indexes

    def _set_dataloader(self, dataset: Dataset, set_id: SetId):
        data_loader = DataLoader(
            dataset,
            batch_size=self._batch_size,
            shuffle=self._shuffle,
            drop_last=self._drop_last,
            num_workers=self._num_workers,
            worker_init_fn=seed_worker,
        )
        set_id = SetId.INFER if set_id == SetId.TEST else set_id
        self._data_loaders[set_id] = data_loader

    def _set_num_classes(self, targets: Tensor):
        self._num_classes = len(torch.unique(targets))

    def _prepare_fold(
            self, dataset: Union[torchvision.datasets.MNIST, torchvision.datasets.SVHN],
            dataset_name: str
    ):
        kf = StratifiedKFold(
            n_splits=self._num_folds,
            shuffle=self._shuffle,
            random_state=self._seed
        )
        if dataset_name == &#39;MNIST&#39;:
            X, y = dataset.data, dataset.targets
        else:
            X, y = dataset.data, dataset.labels
        kf_g = kf.split(X, y)
        for _ in range(1, self._fold): next(kf_g)
        train_index, valid_index = next(kf.split(X, y))
        return train_index, valid_index

    def get_all_loaders(self):
        &#34;&#34;&#34;
        Return all loaders
        &#34;&#34;&#34;
        return super().get_all_loaders()

    def get_cv_loaders(self):
        &#34;&#34;&#34;
        Return all cross-validation loaders
        &#34;&#34;&#34;
        return super().get_cv_loaders()

    def get_loader(self, set_id: SetId):
        &#34;&#34;&#34;
        Get loader by set_id
        Args:
            set_id:
        Return:
            Loader by set_id
        &#34;&#34;&#34;
        return super().get_loader(set_id)

    def num_classes(self):
        return super().num_classes

    @staticmethod
    def _rand_match_on_idx(l1, idx1, l2, idx2, max_d: int = 10000, dm: int = 10):
        &#34;&#34;&#34;
        l*: sorted labels
        idx*: indices of sorted labels in original list
        &#34;&#34;&#34;
        _idx1, _idx2 = [], []
        for l in l1.unique():  # assuming both have same idxs
            l_idx1, l_idx2 = idx1[l1 == l], idx2[l2 == l]
            n = min(l_idx1.size(0), l_idx2.size(0), max_d)
            l_idx1, l_idx2 = l_idx1[:n], l_idx2[:n]
            for _ in range(dm):
                _idx1.append(l_idx1[torch.randperm(n)])
                _idx2.append(l_idx2[torch.randperm(n)])
        return torch.cat(_idx1), torch.cat(_idx2)

    def _download_dataset(self, dataset_dir: str):
        max_d = 10000  # maximum number of datapoints per class
        dm = 30  # data multiplier: random permutations to match

        # get the individual datasets
        tx = torchvision.transforms.ToTensor()
        if os.path.exists(self._dataset_dir):
            download = False
        else:
            download = True
            os.mkdir(self._dataset_dir)
        # load mnist
        train_mnist = torchvision.datasets.MNIST(
            dataset_dir, train=True, download=download, transform=tx)
        test_mnist = torchvision.datasets.MNIST(
            dataset_dir, train=False, download=download, transform=tx)

        # load svhn
        train_svhn = torchvision.datasets.SVHN(
            self._dataset_dir,
            split=&#34;train&#34;, download=download, transform=tx)
        test_svhn = torchvision.datasets.SVHN(
            self._dataset_dir,
            split=SetId.TEST, download=download, transform=tx)

        # svhn labels need extra work
        train_svhn.labels = torch.LongTensor(
            train_svhn.labels.squeeze().astype(int)) % 10
        test_svhn.labels = torch.LongTensor(
            test_svhn.labels.squeeze().astype(int)) % 10

        # split on cross-validation folds
        mnist_train_idxs, mnist_valid_idxs = self._prepare_fold(train_mnist, &#39;MNIST&#39;)
        svhn_train_idxs, svhn_valid_idxs = self._prepare_fold(train_svhn, &#39;SVHN&#39;)

        # save and pair training set
        mnist_l, mnist_li = train_mnist.targets[mnist_train_idxs].sort()
        svhn_l, svhn_li = train_svhn.labels[svhn_train_idxs].sort()
        idx1, idx2 = self._rand_match_on_idx(
            mnist_l, mnist_li, svhn_l, svhn_li, max_d=max_d, dm=dm)
        torch.save(idx1, os.path.join(
            dataset_dir, f&#34;train-ms-mnist-idx-{self._fold}.pt&#34;))
        torch.save(idx2, os.path.join(
            dataset_dir, f&#34;train-ms-svhn-idx-{self._fold}.pt&#34;))
        torch.save(mnist_train_idxs,
                   os.path.join(
                       dataset_dir, f&#39;train-ms-mnist-cv-idx-{self._fold}.pt&#39;))
        torch.save(svhn_train_idxs,
                   os.path.join(
                       dataset_dir, f&#39;train-ms-svhn-cv-idx-{self._fold}.pt&#39;))

        # save and pair validation set
        mnist_l, mnist_li = train_mnist.targets[mnist_valid_idxs].sort()
        svhn_l, svhn_li = train_svhn.labels[svhn_valid_idxs].sort()
        idx1, idx2 = self._rand_match_on_idx(
            mnist_l, mnist_li, svhn_l, svhn_li, max_d=max_d, dm=dm)
        torch.save(idx1, os.path.join(
            dataset_dir, f&#39;valid-ms-mnist-idx-{self._fold}.pt&#39;))
        torch.save(idx2, os.path.join(
            dataset_dir, f&#39;valid-ms-svhn-idx-{self._fold}.pt&#39;))
        torch.save(mnist_valid_idxs,
                   os.path.join(
                       dataset_dir, f&#39;valid-ms-mnist-cv-idx-{self._fold}.pt&#39;))
        torch.save(svhn_valid_idxs,
                   os.path.join(
                       dataset_dir,f&#39;valid-ms-svhn-cv-idx-{self._fold}.pt&#39;))

        # save and pair test set
        mnist_l, mnist_li = test_mnist.targets.sort()
        svhn_l, svhn_li = test_svhn.labels.sort()
        idx1, idx2 = self._rand_match_on_idx(
            mnist_l, mnist_li, svhn_l, svhn_li, max_d=max_d, dm=dm)
        torch.save(idx1, os.path.join(dataset_dir, &#39;test-ms-mnist-idx.pt&#39;))
        torch.save(idx2, os.path.join(dataset_dir, &#39;test-ms-svhn-idx.pt&#39;))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fusion.dataset.abasedataset.ABaseDataset" href="../abasedataset.html#fusion.dataset.abasedataset.ABaseDataset">ABaseDataset</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.get_all_loaders"><code class="name flex">
<span>def <span class="ident">get_all_loaders</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return all loaders</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_all_loaders(self):
    &#34;&#34;&#34;
    Return all loaders
    &#34;&#34;&#34;
    return super().get_all_loaders()</code></pre>
</details>
</dd>
<dt id="fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.get_cv_loaders"><code class="name flex">
<span>def <span class="ident">get_cv_loaders</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return all cross-validation loaders</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_cv_loaders(self):
    &#34;&#34;&#34;
    Return all cross-validation loaders
    &#34;&#34;&#34;
    return super().get_cv_loaders()</code></pre>
</details>
</dd>
<dt id="fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.get_loader"><code class="name flex">
<span>def <span class="ident">get_loader</span></span>(<span>self, set_id: <a title="fusion.dataset.abasedataset.SetId" href="../abasedataset.html#fusion.dataset.abasedataset.SetId">SetId</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Get loader by set_id</p>
<h2 id="args">Args</h2>
<p>set_id:</p>
<h2 id="return">Return</h2>
<p>Loader by set_id</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_loader(self, set_id: SetId):
    &#34;&#34;&#34;
    Get loader by set_id
    Args:
        set_id:
    Return:
        Loader by set_id
    &#34;&#34;&#34;
    return super().get_loader(set_id)</code></pre>
</details>
</dd>
<dt id="fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to load dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self):
    &#34;&#34;&#34;
    Method to load dataset
    &#34;&#34;&#34;
    if os.path.exists(self._dataset_dir):
        self._download_dataset(self._dataset_dir)
    self._num_classes = 10
    # Don&#39;t touch it, otherwise lazy evaluation and lambda functions will make you cry
    samplers = {
        &#39;mnist&#39;: {
            SetId.TRAIN: lambda d, i: self._indexes[SetId.TRAIN][&#39;mnist&#39;][i],
            SetId.VALID: lambda d, i: self._indexes[SetId.VALID][&#39;mnist&#39;][i],
            SetId.TEST: lambda d, i: self._indexes[SetId.TEST][&#39;mnist&#39;][i],
        },
        &#39;svhn&#39;: {
            SetId.TRAIN: lambda d, i: self._indexes[SetId.TRAIN][&#39;svhn&#39;][i],
            SetId.VALID: lambda d, i: self._indexes[SetId.VALID][&#39;svhn&#39;][i],
            SetId.TEST: lambda d, i: self._indexes[SetId.TEST][&#39;svhn&#39;][i],
        }
    }

    for set_id in [SetId.TRAIN, SetId.VALID, SetId.TEST]:
        dataset = None
        sampler_mnist = samplers[&#39;mnist&#39;][set_id]
        sampler_svhn = samplers[&#39;svhn&#39;][set_id]
        if len(self._sources) == 2:
            dataset_mnist, indexes_mnist = self._load(set_id, &#39;mnist&#39;)
            dataset_svhn, indexes_svhn = self._load(set_id, &#39;svhn&#39;)
            self._indexes[set_id] = {}
            self._indexes[set_id][&#39;mnist&#39;] = indexes_mnist
            self._indexes[set_id][&#39;svhn&#39;] = indexes_svhn
            dataset = TensorDataset([
                ResampleDataset(
                    dataset_mnist.dataset,
                    sampler_mnist,
                    size=len(self._indexes[set_id][&#39;mnist&#39;])
                ),
                ResampleDataset(
                    dataset_svhn.dataset,
                    sampler_svhn,
                    size=len(self._indexes[set_id][&#39;svhn&#39;])
                )
            ])
            # collate_fn or tensor dataset with transforms
        else:
            if self._sources[0] == 0:
                dataset_mnist, indexes_mnist = self._load(set_id, &#39;mnist&#39;)
                self._indexes[set_id] = {}
                self._indexes[set_id][&#39;mnist&#39;] = indexes_mnist
                dataset = TensorDataset([
                    ResampleDataset(
                        dataset_mnist.dataset,
                        sampler_mnist,
                        size=len(indexes_mnist)
                    ),
                ])
            elif self._sources[0] == 1:
                self._indexes[set_id] = {}
                dataset_svhn, indexes_svhn = self._load(set_id, &#39;svhn&#39;)
                self._indexes[set_id][&#39;svhn&#39;] = indexes_svhn
                dataset = TensorDataset([
                    ResampleDataset(
                        dataset_svhn.dataset,
                        sampler_svhn,
                        size=len(indexes_svhn)
                    )
                ])
        self._set_dataloader(dataset, set_id)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fusion.dataset.abasedataset.ABaseDataset" href="../abasedataset.html#fusion.dataset.abasedataset.ABaseDataset">ABaseDataset</a></b></code>:
<ul class="hlist">
<li><code><a title="fusion.dataset.abasedataset.ABaseDataset.num_classes" href="../abasedataset.html#fusion.dataset.abasedataset.ABaseDataset.num_classes">num_classes</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fusion.dataset.mnist_svhn" href="index.html">fusion.dataset.mnist_svhn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn" href="#fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn">MnistSvhn</a></code></h4>
<ul class="">
<li><code><a title="fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.get_all_loaders" href="#fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.get_all_loaders">get_all_loaders</a></code></li>
<li><code><a title="fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.get_cv_loaders" href="#fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.get_cv_loaders">get_cv_loaders</a></code></li>
<li><code><a title="fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.get_loader" href="#fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.get_loader">get_loader</a></code></li>
<li><code><a title="fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.load" href="#fusion.dataset.mnist_svhn.mnist_svhn.MnistSvhn.load">load</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>